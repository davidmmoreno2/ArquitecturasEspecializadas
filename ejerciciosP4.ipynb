{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 20:31:10 WARN Utils: Your hostname, david resolves to a loopback address: 127.0.1.1; using 192.168.1.13 instead (on interface wlp1s0)\n",
      "24/11/01 20:31:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 20:31:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 10 países con más casos son:\n",
      "[('United States of America', 103436829), ('China', 99380901), ('India', 45043948), ('France', 39018151), ('Germany', 38437756), ('Brazil', 37511921), ('Republic of Korea', 34571873), ('Japan', 33803572), ('Italy', 26826486), ('United Kingdom of Great Britain and Northern Ireland', 25003394)]\n"
     ]
    }
   ],
   "source": [
    "#1.- Mostrar los 10 países con más casos.\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"ejerciciocovid\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "datos_rd = sc.textFile(\"WHO-COVID-19-global-data.csv\")\n",
    "datos_rd = datos_rd.map(lambda x: x.split(\",\"))\n",
    "datos_rd = datos_rd.filter(lambda x: x[2] != \"\")\n",
    "datos_rd = datos_rd.filter(lambda x: x[4] != \"\")\n",
    "datos_rd = datos_rd.filter(lambda x: len(x) == 8)\n",
    "datos_rd = datos_rd.map(lambda x: (x[2], int (x[4])))\n",
    "datos_rd = datos_rd.reduceByKey(lambda x, y: x + y)\n",
    "datos_rd = datos_rd.sortBy(lambda x: x[1], ascending=False)\n",
    "datos_finales = datos_rd.take(10)\n",
    "\n",
    "print(\"Los 10 países con más casos son:\")\n",
    "print(datos_finales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 20:40:06 WARN Utils: Your hostname, david resolves to a loopback address: 127.0.1.1; using 192.168.1.13 instead (on interface wlp1s0)\n",
      "24/11/01 20:40:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 20:40:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 10 países con más muertes son:\n",
      "[('AMRO', 3034459), ('EURO', 2275741), ('SEARO', 808848), ('WPRO', 421551), ('EMRO', 346267), ('AFRO', 174423), ('OTHER', 13)]\n"
     ]
    }
   ],
   "source": [
    "#2.- Mostrar los continentes ordenados por muertos.\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"ejerciciocovid\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "datos_rd = sc.textFile(\"WHO-COVID-19-global-data.csv\")\n",
    "datos_rd = datos_rd.map(lambda x: x.split(\",\"))\n",
    "datos_rd = datos_rd.filter(lambda x: x[3] != \"\")\n",
    "datos_rd = datos_rd.filter(lambda x: x[6] != \"\")\n",
    "datos_rd = datos_rd.filter(lambda x: len(x) == 8)\n",
    "datos_rd = datos_rd.map(lambda x: (x[3], int (x[6])))\n",
    "datos_rd = datos_rd.reduceByKey(lambda x, y: x + y)\n",
    "datos_rd = datos_rd.sortBy(lambda x: x[1], ascending=False)\n",
    "datos_finales = datos_rd.collect()\n",
    "\n",
    "print(\"Los 10 países con más muertes son:\")\n",
    "print(datos_finales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 20:42:40 WARN Utils: Your hostname, david resolves to a loopback address: 127.0.1.1; using 192.168.1.13 instead (on interface wlp1s0)\n",
      "24/11/01 20:42:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 20:42:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El día con más casos es:\n",
      "[('2022-12-25', 44236172)]\n"
     ]
    }
   ],
   "source": [
    "#3.- Mostrar el día con más casos.\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"ejerciciocovid\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "datos_rd = sc.textFile(\"WHO-COVID-19-global-data.csv\")\n",
    "datos_rd = datos_rd.map(lambda x: x.split(\",\"))\n",
    "datos_rd = datos_rd.filter(lambda x: x[0] != \"\")\n",
    "datos_rd = datos_rd.filter(lambda x: x[4] != \"\")\n",
    "datos_rd = datos_rd.filter(lambda x: len(x) == 8)\n",
    "datos_rd = datos_rd.map(lambda x: (x[0], int (x[4])))\n",
    "datos_rd = datos_rd.reduceByKey(lambda x, y: x + y)\n",
    "datos_rd = datos_rd.sortBy(lambda x: x[1], ascending=False)\n",
    "datos_finales = datos_rd.take(1)\n",
    "\n",
    "print(\"El día con más casos es:\")\n",
    "print(datos_finales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 20:53:44 WARN Utils: Your hostname, david resolves to a loopback address: 127.0.1.1; using 192.168.1.13 instead (on interface wlp1s0)\n",
      "24/11/01 20:53:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/01 20:53:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El día con más muertes es:\n",
      "[('2021-01-24', 103629)]\n"
     ]
    }
   ],
   "source": [
    "#4.- Mostrar el día con más muertos.\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"ejerciciocovid\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "datos_rd = sc.textFile(\"WHO-COVID-19-global-data.csv\")\n",
    "datos_rd = datos_rd.map(lambda x: x.split(\",\"))\n",
    "datos_rd = datos_rd.filter(lambda x: x[6] != \"\")\n",
    "datos_rd = datos_rd.filter(lambda x: len(x) == 8)\n",
    "datos_rd = datos_rd.map(lambda x: (x[0], int (x[6])))\n",
    "datos_rd = datos_rd.reduceByKey(lambda x, y: x + y)\n",
    "datos_rd = datos_rd.sortBy(lambda x: x[1], ascending=False)\n",
    "datos_finales = datos_rd.take(1)\n",
    "\n",
    "print(\"El día con más muertes es:\")\n",
    "print(datos_finales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/02 12:32:43 WARN Utils: Your hostname, david resolves to a loopback address: 127.0.1.1; using 192.168.1.13 instead (on interface wlp1s0)\n",
      "24/11/02 12:32:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/02 12:32:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 10 países con más tasa de casos son:\n",
      "[('Cases_on_an_international_conveyance_Japan', 0.23199999999999998), ('San_Marino', 0.015924226727837797), ('Andorra', 0.009609640807209831), ('Holy_See', 0.009000000000000001), ('Luxembourg', 0.006126095885001185), ('Iceland', 0.0050682459683121526), ('Spain', 0.004443864296933879), ('Gibraltar', 0.0041817426893647315), ('Belgium', 0.004039023406269339), ('Ireland', 0.003968677487984975)]\n"
     ]
    }
   ],
   "source": [
    "#5.- Mostrar 10 los países con más tasa de casos. Paraeste punto, hay que dividir la suma de casos totales\n",
    "#entre la población de cada país.\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"ejerciciocovid\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "datos_rdd = sc.textFile(\"covid19_2.csv\")\n",
    "datos_rdd = datos_rdd.map(lambda x: x.split(\",\"))\n",
    "datos_rdd = datos_rdd.filter(lambda x: len(x) == 11)\n",
    "datos_rdd = datos_rdd.filter(lambda x:x[4] != \"\" and x[9] != \"\")\n",
    "\n",
    "datos_rdd = datos_rdd.map(lambda x: (x[6],(int(x[4])/int(x[9]))))\n",
    "datos_rdd = datos_rdd.reduceByKey(lambda x, y: x + y)\n",
    "datos_rdd = datos_rdd.sortBy(lambda x: x[1], ascending = False)\n",
    "datos_finales = datos_rdd.take(10)\n",
    "\n",
    "print(\"Los 10 países con más tasa de casos son:\")\n",
    "print(datos_finales)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
